{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OKulu5ZTD9Za"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kkyFAnaZobOP"
      },
      "outputs": [],
      "source": [
        "!pip install wandb -qqq\n",
        "import wandb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1m_3WUVBEEFP",
        "outputId": "6717aeb3-5cf5-4796-9043-28b996b13da4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Getting Data...\n",
            "Train  Data Fetched Successfully...\n",
            "Getting Data...\n",
            "Test  Data Fetched Successfully...\n"
          ]
        }
      ],
      "source": [
        "from sklearn.decomposition import PCA\n",
        "import numpy as np\n",
        "from keras.datasets import fashion_mnist\n",
        "from sklearn.model_selection import train_test_split\n",
        "from scipy.special import expit, logit\n",
        "import pandas as pd\n",
        "\n",
        "#One_Hot_Encoding():convert the given Data in one hot encoding in a 2D array \n",
        "def One_Hot_Encoding(Y, n_classes):\n",
        "    src_arr = np.zeros((Y.shape[0],n_classes))\n",
        "    for i in range(Y.shape[0]):\n",
        "        Temp = [0.0]*n_classes\n",
        "        Temp[Y[i]] = 1.0\n",
        "        src_arr[i] = Temp\n",
        "    return src_arr\n",
        "\n",
        "\n",
        "#Get_Data(): To fetch Train/Test Data from the given file\n",
        "def Get_Data(file, data, normalise = True, mean=0, std=1):\n",
        "    print('Getting Data...')\n",
        "    n_classes = 10\n",
        "    \n",
        "    df = pd.read_csv(file)\n",
        "    \n",
        "    df = df.iloc[np.random.permutation(len(df))]\n",
        "  \n",
        "    X = df[df.columns[0:-1]]\n",
        "  \n",
        "    \n",
        "    mean = X.mean(axis=0)\n",
        "    std  = X.std(axis = 0)\n",
        "    \n",
        "    if normalise == True:\n",
        "      X = Normalise_Data(X, mean, std)\n",
        "    \n",
        "    X = X.values\n",
        "  \n",
        "    Y = df['label'].values\n",
        "    \n",
        "    \n",
        "    print(data,' Data Fetched Successfully...')\n",
        "\n",
        "    return (X, One_Hot_Encoding(Y, n_classes), mean, std)\n",
        "#Normalise_Data():Normalizing the Given data\n",
        "def Normalise_Data(data, mean, std):\n",
        "    data = data - mean\n",
        "    data = data/std\n",
        "    return data\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "(X_train, Y_train, train_mean, train_std)=Get_Data('/content/drive/MyDrive/fashion/fashion-mnist_train.csv', data = 'Train', normalise = True )\n",
        "\n",
        "X_Train, X_Valid, Y_Train, Y_Valid = train_test_split(X_train, Y_train, test_size=0.10, random_state=42)\n",
        "(X_Test, Y_Test, train_mean, train_std)=Get_Data('/content/drive/MyDrive/fashion/fashion-mnist_test.csv', data = 'Test', normalise = True )\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eVmu-uysN9E9"
      },
      "outputs": [],
      "source": [
        "import wandb\n",
        "from re import X\n",
        "import numpy as np\n",
        "import warnings\n",
        "from sklearn.metrics import log_loss\n",
        "from scipy.special import expit, logit\n",
        "from sklearn.metrics import mean_squared_error\n",
        "warnings.filterwarnings(\"ignore\", category=np.VisibleDeprecationWarning) \n",
        "\n",
        "#Class that initializes the initial parameters and contain the main operating functions  \n",
        "class Feed_Forward_NeuralNetwork(object):\n",
        "  number_of_neurons=[]\n",
        "  number_of_layers=0\n",
        "  Biases=[]\n",
        "  Weights=[]\n",
        "  Activation_gx=[]\n",
        "  Layer_Output=[]\n",
        "  Layer_Input=[]\n",
        "  Loss_Function=\"\"\n",
        "  #Default fucntion to initialize parameters \n",
        "  def __init__(self,Layers,Activations,Loss_Funciton=\"CE\",seed=8790,Weight_init='Random'):\n",
        "    self.number_of_layers=len(Layers)\n",
        "    self.number_of_neurons=Layers\n",
        "    self.Activation_gx=Activations\n",
        "    self.Loss_Function=Loss_Funciton\n",
        "    if Weight_init=='Xavier':\n",
        "      self.Weights=[np.random.randn(sizej,sizei)*np.sqrt(1/(sizei)) for sizei,sizej in zip(self.number_of_neurons[:-1],self.number_of_neurons[1:])]  \n",
        "    else:\n",
        "      self.Weights=[np.random.randn(sizej,sizei)*np.sqrt(2/(sizei+sizej)) for sizei,sizej in zip(self.number_of_neurons[:-1],self.number_of_neurons[1:])]\n",
        "    self.Biases=[np.zeros((x,1)) for x in self.number_of_neurons[1:]]\n",
        "    \n",
        "\n",
        "  #Definition of alll the Activation Fucntion and there Derivatives\n",
        "  def Sigmoid(self,x):\n",
        "    return  expit(x)\n",
        "  def stable_softmax(self,x):\n",
        "    out = np.zeros(x.shape)\n",
        "    for i in range(0, x.shape[1]):\n",
        "      exps = np.exp(x[:, i] - np.max(x[:, i]))\n",
        "      out[:, i] = exps / np.sum( exps)\n",
        "    return out\n",
        "  def Sigmoid_Drv(self, x):\n",
        "    sigma_x = self.Sigmoid(x)\n",
        "    return (sigma_x * (1 - sigma_x))\n",
        "  def Relu(self,x):\n",
        "    return np.maximum(0,x,x)\n",
        "  def Tanh(self,x):\n",
        "    return np.tanh(x)\n",
        "  def Tanh_Drv(self, x):\n",
        "    tanh_x = np.tanh(x)\n",
        "    return (1 - np.square(tanh_x))\n",
        "  def Relu_Drv(self, x):\n",
        "    return np.greater(x,0).astype(int)\n",
        "\n",
        "  #Feed_Network():Function that Implements Feed Forward Neural Network\n",
        "  def Feed_Network(self,X_train):\n",
        "    self.Layer_Input=[]\n",
        "    self.Layer_Output=[]\n",
        "    Weights=self.Weights\n",
        "    biases=self.Biases\n",
        "    no_of_datapoints=X_train.shape[0]\n",
        "    no_of_features=X_train.shape[1]\n",
        "    Temp=[]\n",
        "    Activations=[]\n",
        "    #Loop through all the layer and set the parameters\n",
        "    for i in range(0,self.number_of_layers-1):\n",
        "\n",
        "      if i==0:\n",
        "        Temp.append(np.matmul(Weights[i],X_train.T)+biases[i])\n",
        "      else:\n",
        "        Temp.append(np.matmul(Weights[i],Activations[i-1])+biases[i])\n",
        "      \n",
        "      self.Layer_Input.append(Temp[i])\n",
        "\n",
        "      #Check For Activation Fucnction\n",
        "      if (self.Activation_gx[i]=='SoftMax') and (i == self.number_of_layers-2):\n",
        "        Activations.append(self.stable_softmax(Temp[i]))\n",
        "      elif self.Activation_gx[i]=='Tanh':\n",
        "        Activations.append(self.Tanh(Temp[i]))\n",
        "      elif self.Activation_gx[i]=='Sigmoid':\n",
        "        Activations.append(self.Sigmoid(Temp[i]))\n",
        "      elif self.Activation_gx[i]=='Relu':\n",
        "        Activations.append(self.Relu(Temp[i]))\n",
        "      \n",
        "\n",
        "\n",
        "\n",
        "      self.Layer_Output.append(Activations[i])\n",
        "    #Return the Probability distribution over the classes\n",
        "    return self.Layer_Output[-1]\n",
        "  #Backward_Pass():Implements Back Propogation Algorithm\n",
        "  def Backward_Pass(self,X_train,Y_train):\n",
        "    Gredient_wrt_weight=[0]*(len(self.Weights))\n",
        "    Gradient_wrt_biases=[0]*(len(self.Biases))\n",
        "    Gradient_wrt_layers=[0]*(self.number_of_layers-1)\n",
        "    n=X_train.shape[0]\n",
        "\n",
        "\n",
        "    for i in reversed(range(self.number_of_layers-1)):\n",
        "      #Backdrop\n",
        "      if i == self.number_of_layers-2:\n",
        "        if self.Loss_Function=='CE':\n",
        "          Gradient_wrt_layers[i]=self.Layer_Output[i]-Y_train.T\n",
        "        elif self.Loss_Function=='MSE':\n",
        "          Gradient_wrt_layers[i]=(self.Layer_Output[i]-Y_train.T)*self.Layer_Output[i]*(1-self.Layer_Output[i])\n",
        "\n",
        "        Gredient_wrt_weight[i]=(1/n)*np.matmul(Gradient_wrt_layers[i],self.Layer_Output[i-1].T)\n",
        "        Gradient_wrt_biases[i]=(1/n)*np.sum(Gradient_wrt_layers[i],axis=1,keepdims=True)\n",
        "      #For the remaining layers other than the output layer\n",
        "      else:\n",
        "        activation_derivative=[]\n",
        "        if self.Activation_gx[i]=='Tanh':\n",
        "          activation_derivative=self.Tanh_Drv(Gradient_wrt_layers[i])\n",
        "        elif self.Activation_gx[i]=='Sigmoid':\n",
        "          activation_derivative=self.Sigmoid_Drv(Gradient_wrt_layers[i])\n",
        "        elif self.Activation_gx[i]=='Relu':\n",
        "          activation_derivative=self.Relu_Drv(Gradient_wrt_layers[i])\n",
        "        \n",
        "        Gradient_wrt_layers[i]=np.matmul(self.Weights[i+1].T,Gradient_wrt_layers[i+1])*activation_derivative#self.Tanh_Drv(Gradient_wrt_layers[i])\n",
        "        \n",
        "        if i==0:\n",
        "          Gredient_wrt_weight[i]=(1/n)*np.matmul(Gradient_wrt_layers[i],X_train)\n",
        "        else:\n",
        "          Gredient_wrt_weight[i]=(1/n)*np.matmul(Gradient_wrt_layers[i],self.Layer_Output[i-1].T)\n",
        "        \n",
        "        Gradient_wrt_biases[i]=(1/n)*np.sum(Gradient_wrt_layers[i],axis=1,keepdims=True)\n",
        "    # Return the gradient wrt to weights , biases ,layers\n",
        "    return (Gradient_wrt_biases,Gredient_wrt_weight,Gradient_wrt_layers)\n",
        "  #Compute the accuracy \n",
        "  def Get_Accuracy(self,y,y_pred):\n",
        "    P=np.argmax(y,axis=0)\n",
        "    Q=np.argmax(y_pred,axis=0)\n",
        "    acc=np.sum(P == Q)/len(P)\n",
        "    return 100*acc\n",
        "  #Optimizer Fucntions definitions\n",
        "  def GD(self,Grd_w,Grd_b,Eta):\n",
        "    return (np.multiply(Eta,Grd_w),np.multiply(Eta,Grd_b))\n",
        "  def Momentum(self,Momentum,Grd_w,Grd_b,Eta,prev_w,prev_b):\n",
        "    upd_weight=np.multiply(Momentum,prev_w)+np.multiply(Eta,Grd_w)\n",
        "    upd_biases=np.multiply(Momentum,prev_b)+np.multiply(Eta,Grd_b)\n",
        "    return (upd_weight,upd_biases)\n",
        "  def RMSprop(self,Grd_w,Grd_b,Eta):\n",
        "    w_v,b_v=0,0\n",
        "    eps=1e-3\n",
        "    beta=0.9\n",
        "    w_v=np.multiply(beta,w_v)\n",
        "    b_v=np.multiply(beta,b_v)\n",
        "    w_v=w_v+np.multiply(1-beta,np.power(Grd_w,2))\n",
        "    b_v=b_v+np.multiply(1-beta,np.power(Grd_b,2))\n",
        "    w_v_corrected=1/(np.power(w_v+eps,0.5))\n",
        "    b_v_corrected=1/(np.power(b_v+eps,0.5))\n",
        "    upd_w=np.multiply(w_v_corrected,Grd_w)*Eta\n",
        "    upd_b=np.multiply(b_v_corrected,Grd_b)*Eta\n",
        "    return (upd_w,upd_b)\n",
        "  def Adam(self,Grd_w,Grd_b,Eta,t):\n",
        "    w_m,b_m=0,0\n",
        "    w_v,b_v=0,0\n",
        "    beta1,beta2=0.9,0.99\n",
        "    eps=1e-8\n",
        "    w_v=np.multiply(beta2,w_v)\n",
        "    b_v=np.multiply(beta2,b_v)\n",
        "    w_v=w_v+np.multiply(1-beta2,np.power(Grd_w,2))\n",
        "    b_v=b_v+np.multiply(1-beta2,np.power(Grd_b,2))\n",
        "    w_v=w_v/(1-np.power(beta2,t+1))\n",
        "    b_v=b_v/(1-np.power(beta2,t+1))\n",
        "    w_m=np.multiply(beta1,w_m)\n",
        "    b_m=np.multiply(beta1,b_m)\n",
        "    w_m=w_m+np.multiply(1-beta1,Grd_w)\n",
        "    b_m=b_m+np.multiply(1-beta1,Grd_b)\n",
        "    w_m=w_m/(1-np.power(beta1,t+1))\n",
        "    b_m=b_m/(1-np.power(beta1,t+1))\n",
        "    upd_w=(Eta/np.power(w_v+eps,0.5))*w_m\n",
        "    upd_b=(Eta/np.power(b_v+eps,0.5))*b_m\n",
        "\n",
        "    return (upd_w,upd_b)\n",
        "  def Nadam(self,Grd_w,Grd_b,Eta,t):\n",
        "    w_m,b_m=0,0\n",
        "    w_v,b_v=0,0\n",
        "    beta1,beta2=0.9,0.99\n",
        "    eps=1e-8\n",
        "    w_v=np.multiply(beta2,w_v)\n",
        "    b_v=np.multiply(beta2,b_v)\n",
        "    w_v=w_v+np.multiply(1-beta2,np.power(Grd_w,2))\n",
        "    b_v=b_v+np.multiply(1-beta2,np.power(Grd_b,2))\n",
        "    w_m=np.multiply(beta1,w_m)\n",
        "    b_m=np.multiply(beta1,b_m)\n",
        "    w_m=w_m+np.multiply(1-beta1,Grd_w)\n",
        "    b_m=b_m+np.multiply(1-beta1,Grd_b)\n",
        "    w_m=np.multiply(beta1,w_m)/(1-beta1)+np.multiply(beta1,Grd_w)/(1-beta1)\n",
        "    b_m=np.multiply(beta1,b_m)/(1-beta1)+np.multiply(beta1,Grd_b)/(1-beta1)\n",
        "    w_v=np.multiply(np.multiply(beta2,w_v),1/(1-beta2))\n",
        "    b_v=np.multiply(np.multiply(beta2,b_v),1/(1-beta2))\n",
        "    upd_w=(Eta/np.power(w_v+eps,0.5))*w_m\n",
        "    upd_b=(Eta/np.power(b_v+eps,0.5))*b_m \n",
        "                                \n",
        "    return (upd_w,upd_b)\n",
        "  def Nag(self,gamma,Grd_w,Grd_b,Eta,prev_w,prev_b):\n",
        "    w_v,b_v=0,0\n",
        "    w_v=np.multiply(gamma,prev_w)+np.multiply(Eta,Grd_w)\n",
        "    b_v=np.multiply(gamma,prev_b)+np.multiply(Eta,Grd_b)\n",
        "    prev_w=w_v \n",
        "    prev_b=b_v\n",
        "    w_v=np.multiply(gamma,prev_w)\n",
        "    b_v=np.multiply(gamma,prev_b)\n",
        "    return w_v,b_v\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  #Main_Training_Algo():It combines Feed forward neural Network and Back Propogation with optimizers\n",
        "  def Main_Training_Algo(self,X_Valid,Y_Valid,X_Train,Y_Train,optimizer='Momentum',NEpochs=5,Batch_Size=16,Momentum=0.9,Eta=0.01,anneal=False):\n",
        "\n",
        "    etap=Eta\n",
        "    Loss_in_Validation=[]\n",
        "    Loss_in_Training=[]\n",
        "    update_weight=[]\n",
        "    update_biases=[]\n",
        "    prev_w=[np.zeros((sizej,sizei)) for sizei,sizej in zip(self.number_of_neurons[:-1],self.number_of_neurons[1:])]\n",
        "    prev_b=[np.zeros((x,1)) for x in self.number_of_neurons[1:]]\n",
        "    t=0\n",
        "    for Ep in range(NEpochs):\n",
        "      Loss_in_batch=0\n",
        "\n",
        "      for start in range(0,X_Train.shape[0],Batch_Size):\n",
        "\n",
        "        X_Train_curr_batch=X_Train[start:start+Batch_Size]\n",
        "        Y_Train_curr_batch=Y_Train[start:start+Batch_Size]\n",
        "        \n",
        "\n",
        "        Curr_Batch_Prediction=self.Feed_Network(X_Train_curr_batch)\n",
        "\n",
        "        (Gradient_wrt_biases,Gradient_wrt_weight,Gradient_wrt_layers)=self.Backward_Pass(X_Train_curr_batch,Y_Train_curr_batch)\n",
        "\n",
        "\n",
        "        if optimizer=='GD':\n",
        "          update_weight,update_biases=self.GD(Gradient_wrt_weight,Gradient_wrt_biases,Eta)\n",
        "        elif optimizer=='Momentum':\n",
        "          update_weight,update_biases=self.Momentum(Momentum,Gradient_wrt_weight,Gradient_wrt_biases,Eta,prev_w,prev_b)\n",
        "          prev_w=update_weight\n",
        "          prev_b=update_biases\n",
        "        elif optimizer=='RMSprop':\n",
        "          update_weight,update_biases=self.RMSprop(Gradient_wrt_weight,Gradient_wrt_biases,Eta)\n",
        "        elif optimizer=='Adam':\n",
        "          update_weight,update_biases=self.Adam(Gradient_wrt_weight,Gradient_wrt_biases,Eta,t)\n",
        "        elif optimizer=='Nadam':\n",
        "          update_weight,update_biases=self.Nadam(Gradient_wrt_weight,Gradient_wrt_biases,Eta,t)\n",
        "        elif optimizer=='Nag':\n",
        "          update_weight,update_biases=self.Nag(Momentum,Gradient_wrt_weight,Gradient_wrt_biases,Eta,prev_w,prev_b)\n",
        "          prev_w=update_weight\n",
        "          prev_b=update_biases\n",
        "          \n",
        "        t=t+1\n",
        "\n",
        "        \n",
        "        self.Weights=self.Weights-update_weight\n",
        "        self.Biases=self.Biases-update_biases\n",
        "        if anneal == True:\n",
        "          Eta = ((0.99)**(start+3.5))*etap\n",
        "\n",
        "        if self.Loss_Function=='CE':\n",
        "          Loss_in_batch=Loss_in_batch+log_loss(Y_Train_curr_batch.T,Curr_Batch_Prediction)\n",
        "        elif self.Loss_Function=='MSE':\n",
        "          Loss_in_batch=Loss_in_batch+(mean_squared_error(Y_Train_curr_batch.T,Curr_Batch_Prediction))          \n",
        "      \n",
        "\n",
        "      Loss_in_Training.append(Loss_in_batch)\n",
        "      Train_Prediction=self.Feed_Network(X_Train)\n",
        "      print(\"Epoch:\",Ep)\n",
        "      print(\"Train Accuracy\",self.Get_Accuracy(Y_Train.T,Train_Prediction))\n",
        "      print(\"Loss in Training\",Loss_in_Training[Ep])\n",
        "      \n",
        "      Validation_Predcition=self.Feed_Network(X_Valid)\n",
        "      if self.Loss_Function=='CE':\n",
        "        Loss_in_Validation.append(log_loss(Y_Valid.T,Validation_Predcition))\n",
        "      elif self.Loss_Function=='MSE':\n",
        "        Loss_in_Validation.append(mean_squared_error(Y_Valid.T,Validation_Predcition))\n",
        "      print(\"Validation Accuracy\",self.Get_Accuracy(Y_Valid.T,Validation_Predcition))\n",
        "      print(\"Loss in Validation\",Loss_in_Validation[Ep])\n",
        "      wandb.log({\"val_loss\": Loss_in_Training[Ep]/10000, \\\n",
        "                       \"val_acc\": self.Get_Accuracy(Y_Train.T,Train_Prediction), \\\n",
        "                       \"test_loss\": Loss_in_Validation[Ep]/10000, \\\n",
        "                       \"test_acc\": self.Get_Accuracy(Y_Valid.T,Validation_Predcition), \\\n",
        "                       \"epoch\":Ep})\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z8YjI_uCmlcm"
      },
      "outputs": [],
      "source": [
        "def Main_Plots():\n",
        "    sweep_config = {\"name\": \"complete-sweep\", \"method\": \"grid\"}\n",
        "    sweep_config[\"metric\"] = {\"name\": \"accuracy\", \"goal\": \"minimize\"}\n",
        "    parameters_dict = {\n",
        "                    \"num_epochs\": {\"values\": [5,10,20]}, \\\n",
        "                    \"Layer_size\": {\"values\": [32,64,128]}, \\\n",
        "                    \"learning_rate\": {\"values\": [1e-3,1e-4]}, \\\n",
        "                    \"N_hl\": {\"values\": [4]}, \\\n",
        "                    \"optimizer\": {\"values\": [\"Momentum\",\"GD\",\"Nadam\",\"Nag\",\"RMSProp\",\"Adam\"]}, \\\n",
        "                    \"batch_size\": {\"values\": [32,64,128]}, \\\n",
        "                    \"weight_init\": {\"values\": [\"Random\",\"Xavier\"]} , \\\n",
        "                    \"activation\": {\"values\": [ \"Tanh\",\"Sigmoid\",\"Relu\"]}, \\\n",
        "                    \"loss\": {\"values\": [\"CE\",\"MSE\"]}, \\\n",
        "                      }\n",
        "    sweep_config[\"parameters\"] = parameters_dict\n",
        "    global accuracy\n",
        "    accuracy=0\n",
        "    def train_nn(config = sweep_config):\n",
        "        global accuracy\n",
        "        with wandb.init(config = config):\n",
        "            config = wandb.init().config\n",
        "            wandb.run.name = \"e_{}_loss_{}_opt_{}_bs_{}_init_{}_ac_{}\".format(config.num_epochs,\\\n",
        "                                                                          config.loss,\\\n",
        "                                                                          config.optimizer,\\\n",
        "                                                                          config.batch_size,\\\n",
        "                                                                          config.weight_init,\\\n",
        "                                                                          config.activation,\\\n",
        "                                                                          )\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "            n_classes = 10\n",
        "            seed =8790\n",
        "            loss_function = config.loss\n",
        "            Activation_gx = [config.activation, config.activation,config.activation,config.activation, 'SoftMax']\n",
        "            neurons=config.Layer_size\n",
        "            Layers = [784, neurons,neurons,neurons,neurons, n_classes]\n",
        "            Network=Feed_Forward_NeuralNetwork(Layers,Activation_gx,loss_function,seed,Weight_init=config.weight_init) \n",
        "            Network.Main_Training_Algo(X_Valid,Y_Valid,X_Train,Y_Train,optimizer=config.optimizer,NEpochs=config.num_epochs,Batch_Size=config.batch_size,Momentum=0.9,Eta=config.learning_rate,anneal=True)\n",
        "            Validation_Predcition=Network.Feed_Network(X_Valid)\n",
        "            accuracy=Network.Get_Accuracy(Y_Valid.T,Validation_Predcition)\n",
        "\n",
        "            wandb.log({\"accuracy\": accuracy})\n",
        "    sweep_id = wandb.sweep(sweep_config, project = \"Main_Plots\")\n",
        "    wandb.agent(sweep_id, function = train_nn)\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DwQ-YBeNmlcn",
        "scrolled": false
      },
      "outputs": [],
      "source": [
        "Main_Plots() #To create all the main plots"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def ConfusionMatrix():\n",
        "  import matplotlib.pyplot as plt\n",
        "\n",
        "  import plotly\n",
        "  import seaborn as sns\n",
        "  from sklearn.metrics import confusion_matrix\n",
        "  sweep_config = {\"name\": \"best-sweep\", \"method\": \"grid\"}\n",
        "  sweep_config[\"metric\"] = {\"name\": \"loss\", \"goal\": \"minimize\"}\n",
        "  parameters_dict = {\n",
        "                    \"num_epochs\": {\"values\": [20]}, \\\n",
        "                    \"Layer_size\": {\"values\": [64]}, \\\n",
        "                    \"learning_rate\": {\"values\": [1e-3]}, \\\n",
        "                    \"N_hl\": {\"values\": [4]}, \\\n",
        "                    \"optimizer\": {\"values\": [\"Nadam\"]}, \\\n",
        "                    \"batch_size\": {\"values\": [64]}, \\\n",
        "                    \"weight_init\": {\"values\": [\"Random\"]} , \\\n",
        "                    \"activation\": {\"values\": [ \"Tanh\"]}, \\\n",
        "                    \"loss\": {\"values\": [\"CE\"]}, \\\n",
        "                      }\n",
        "  sweep_config[\"parameters\"] = parameters_dict\n",
        "  global accuracy\n",
        "  accuracy=0\n",
        "\n",
        "  ####################################################################\n",
        "  def train_nn(config = sweep_config):\n",
        "      global accuracy\n",
        "      with wandb.init(config = config):\n",
        "          config = wandb.init().config\n",
        "          wandb.run.name = \"e_{}_loss_{}_opt_{}_bs_{}_init_{}_ac_{}\".format(config.num_epochs,\\\n",
        "                                                                          config.loss,\\\n",
        "                                                                          config.optimizer,\\\n",
        "                                                                          config.batch_size,\\\n",
        "                                                                          config.weight_init,\\\n",
        "                                                                          config.activation,\\\n",
        "                                                                          )\n",
        "\n",
        "        \n",
        "        \n",
        "        \n",
        "        \n",
        "          n_classes = 10\n",
        "          seed =8790\n",
        "          loss_function = config.loss\n",
        "          Activation_gx = [config.activation, config.activation,config.activation,config.activation, 'SoftMax']\n",
        "          neurons =config.Layer_size\n",
        "          Layers = [784, neurons,neurons ,neurons,neurons, n_classes]\n",
        "          Network=Feed_Forward_NeuralNetwork(Layers,Activation_gx,loss_function,seed,config.weight_init) \n",
        "          Network.Main_Training_Algo(X_Valid,Y_Valid,X_Train,Y_Train,config.optimizer,config.num_epochs,config.batch_size,Momentum=0.9,Eta=config.learning_rate,anneal=True)\n",
        "          Y_test_pred=Network.Feed_Network(X_Test)\n",
        "\n",
        "\n",
        "          E=np.argmax(Y_Test.T,axis=0)\n",
        "          T=np.argmax(Y_test_pred,axis=0)\n",
        "          cf_matrix = confusion_matrix(E,T)\n",
        "          class_names=[\"TshirtðŸ‘•\",\"TrouserðŸ‘–\",\"PulloverðŸ§¥\",\"DressðŸ‘—\",\"CoatðŸ¥¼\",\"SandalðŸ‘¡\",\"ShirtðŸ‘”\",\"SneakerðŸ‘Ÿ\",\"BagðŸ‘œ\",\"AnkleBootðŸ¥¾\"]\n",
        "          wandb.log({\"my_conf_mat_id\" : wandb.plot.confusion_matrix( preds=T, y_true=E,class_names=class_names)})\n",
        "\n",
        "  ####################################################################\n",
        "  sweep_id = wandb.sweep(sweep_config, project = \"Best_Plot\")\n",
        "  wandb.agent(sweep_id, function = train_nn)\n",
        "  #################################################################### [markdown]"
      ],
      "metadata": {
        "id": "QVUWhGklAHoE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ConfusionMatrix()#To create confusion matrix for best Hyperparameter config."
      ],
      "metadata": {
        "id": "fkiBMDecAbMI"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "FFNN_Plots.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}