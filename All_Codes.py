# -*- coding: utf-8 -*-
"""FFNN

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/17KxpW3sKNYC9YfFopoKKyWVfKCZ2xAi6
"""

!pip install wandb -qqq
import wandb

from google.colab import drive
drive.mount('/content/drive')

import numpy as np
from sklearn.model_selection import train_test_split
from scipy.special import expit, logit
import pandas as pd
#One_Hot_Encoding():convert the given Data in one hot encoding in a 2D array 
def One_Hot_Encoding(Y, n_classes):
    src_arr = np.zeros((Y.shape[0],n_classes))
    for i in range(Y.shape[0]):
        Temp = [0.0]*n_classes
        Temp[Y[i]] = 1.0
        src_arr[i] = Temp
    return src_arr

#Get_Data(): To fetch Train/Test Data from the given file
def Get_Data(file, data, normalise = True, mean=0, std=1):
    print('Getting Data...')
    n_classes = 10
    
    df = pd.read_csv(file)
    
    df = df.iloc[np.random.permutation(len(df))]
  
    X = df[df.columns[0:-1]]
  
    
    mean = X.mean(axis=0)
    std  = X.std(axis = 0)
    
    if normalise == True:
      X = Normalise_Data(X, mean, std)
    
    X = X.values
  
    Y = df['label'].values
    
    
    print(data,' Data Fetched Successfully...')

    return (X, One_Hot_Encoding(Y, n_classes), mean, std)
#Normalise_Data():Normalizing the Given data
def Normalise_Data(data, mean, std):
    data = data - mean
    data = data/std
    return data




(X_train, Y_train, train_mean, train_std)=Get_Data('/content/drive/MyDrive/fashion/fashion-mnist_train.csv', data = 'Train', normalise = True )

#Splitting the Train data 
X_Train, X_Valid, Y_Train, Y_Valid = train_test_split(X_train, Y_train, test_size=0.10, random_state=42)

(X_Test, Y_Test, train_mean, train_std)=Get_Data('/content/drive/MyDrive/fashion/fashion-mnist_test.csv', data = 'Test', normalise = True )

#from re import X
import numpy as np
from scipy.special import expit, logit
import warnings
from sklearn.metrics import log_loss
from sklearn.metrics import mean_squared_error
warnings.filterwarnings("ignore", category=np.VisibleDeprecationWarning)

#Class that initializes the initial parameters and contain the main operating functions  
class Feed_Forward_NeuralNetwork(object):
  number_of_neurons=[]
  number_of_layers=0
  Biases=[]
  Weights=[]
  Activation_gx=[]
  Layer_Output=[]
  Layer_Input=[]
  Loss_Function=""
  #Default fucntion to initialize parameters 
  def __init__(self,Layers,Activations,Loss_Funciton="CE",seed=8790,Weight_init='Random'):
    self.number_of_layers=len(Layers)
    self.number_of_neurons=Layers
    self.Activation_gx=Activations
    self.Loss_Function=Loss_Funciton
    if Weight_init=='Xavier':
      self.Weights=[np.random.randn(sizej,sizei)*np.sqrt(1/(sizei)) for sizei,sizej in zip(self.number_of_neurons[:-1],self.number_of_neurons[1:])]  
    else:
      self.Weights=[np.random.randn(sizej,sizei)*np.sqrt(2/(sizei+sizej)) for sizei,sizej in zip(self.number_of_neurons[:-1],self.number_of_neurons[1:])]
    self.Biases=[np.zeros((x,1)) for x in self.number_of_neurons[1:]]
    

  #Definition of alll the Activation Fucntion and there Derivatives
  def Sigmoid(self,x):
    return  expit(x)
  def stable_softmax(self,data):
    out = np.zeros(data.shape)
    for i in range(0, data.shape[1]):
      Exps = np.exp(data[:, i] - np.max(data[:, i]))
      out[:, i] = Exps / np.sum( Exps)
    return out
  def Sigmoid_Drv(self, x):
    sigma_x = self.Sigmoid(x)
    return (sigma_x * (1 - sigma_x))
  def Relu(self,x):
    return np.maximum(0.0,x)
  def Tanh(self,x):
    return np.tanh(x)
  def Tanh_Drv(self, x):
    tanh_x = np.tanh(x)
    return (1 - np.square(tanh_x))
  def Relu_Drv(self, x):
    return np.greater(x,0).astype(int)

  #Feed_Network():Function that Implements Feed Forward Neural Network
  def Feed_Network(self,X_train):
    self.Layer_Input=[]
    self.Layer_Output=[]
    Weights=self.Weights
    biases=self.Biases
    no_of_datapoints=X_train.shape[0]
    no_of_features=X_train.shape[1]
    Temp=[]
    Activations=[]
    #Loop through all the layer and set the parameters
    for i in range(0,self.number_of_layers-1):

      if i==0:
        Temp.append(np.matmul(Weights[i],X_train.T)+biases[i])
      else:
        Temp.append(np.matmul(Weights[i],Activations[i-1])+biases[i])
      
      self.Layer_Input.append(Temp[i])

      #Check For Activation Fucnction

      if (self.Activation_gx[i]=='SoftMax') and (i == self.number_of_layers-2):#for the output layer
        Activations.append(self.stable_softmax(Temp[i]))
      elif self.Activation_gx[i]=='Tanh':
        Activations.append(self.Tanh(Temp[i]))
      elif self.Activation_gx[i]=='Sigmoid':
        Activations.append(self.Sigmoid(Temp[i]))
      elif self.Activation_gx[i]=='Relu':
        Activations.append(self.Relu(Temp[i]))
      



      self.Layer_Output.append(Activations[i])
    #Return the Probability distribution over the classes
    return self.Layer_Output[-1]

  #Backward_Pass():Implements Back Propogation Algorithm
  def Backward_Pass(self,X_train,Y_train):
    Gredient_wrt_weight=[0]*(len(self.Weights))
    Gradient_wrt_biases=[0]*(len(self.Biases))
    Gradient_wrt_layers=[0]*(self.number_of_layers-1)
    n=X_train.shape[0]


    for i in reversed(range(self.number_of_layers-1)):
      #Backdrop 
      if i == self.number_of_layers-2:
        if self.Loss_Function=='CE':
          Gradient_wrt_layers[i]=self.Layer_Output[i]-Y_train.T
        elif self.Loss_Function=='MSE':
          Gradient_wrt_layers[i]=(self.Layer_Output[i]-Y_train.T)*self.Layer_Output[i]*(1-self.Layer_Output[i])

        Gredient_wrt_weight[i]=(1/n)*np.matmul(Gradient_wrt_layers[i],self.Layer_Output[i-1].T)
        Gradient_wrt_biases[i]=(1/n)*np.sum(Gradient_wrt_layers[i],axis=1,keepdims=True)
      #for the remaining layers other than the output layer
      else:
        activation_derivative=[]
        if self.Activation_gx[i]=='Tanh':
          activation_derivative=self.Tanh_Drv(Gradient_wrt_layers[i])
        elif self.Activation_gx[i]=='Sigmoid':
          activation_derivative=self.Sigmoid_Drv(Gradient_wrt_layers[i])
        elif self.Activation_gx[i]=='Relu':
          activation_derivative=self.Relu_Drv(Gradient_wrt_layers[i])
        
        Gradient_wrt_layers[i]=np.matmul(self.Weights[i+1].T,Gradient_wrt_layers[i+1])*activation_derivative#self.Tanh_Drv(Gradient_wrt_layers[i])
        
        if i==0:
          Gredient_wrt_weight[i]=(1/n)*np.matmul(Gradient_wrt_layers[i],X_train)
        else:
          Gredient_wrt_weight[i]=(1/n)*np.matmul(Gradient_wrt_layers[i],self.Layer_Output[i-1].T)
        
        Gradient_wrt_biases[i]=(1/n)*np.sum(Gradient_wrt_layers[i],axis=1,keepdims=True)
    # Return the gradient wrt to weights , biases ,layers
    return (Gradient_wrt_biases,Gredient_wrt_weight,Gradient_wrt_layers)
  #Compute the accuracy 
  def Get_Accuracy(self,y,y_pred):
    P=np.argmax(y,axis=0)
    Q=np.argmax(y_pred,axis=0)
    acc=np.sum(P == Q)/len(P)
    return 100*acc
  #Optimizer Fucntions definitions
  def GD(self,Grd_w,Grd_b,Eta):
    return (np.multiply(Eta,Grd_w),np.multiply(Eta,Grd_b))
  def Momentum(self,Momentum,Grd_w,Grd_b,Eta,prev_w,prev_b):
    upd_weight=np.multiply(Momentum,prev_w)+np.multiply(Eta,Grd_w)
    upd_biases=np.multiply(Momentum,prev_b)+np.multiply(Eta,Grd_b)
    return (upd_weight,upd_biases)
  def RMSprop(self,Grd_w,Grd_b,Eta):
    w_v,b_v=0,0
    eps=1e-3
    beta=0.9
    w_v=np.multiply(beta,w_v)
    b_v=np.multiply(beta,b_v)
    w_v=w_v+np.multiply(1-beta,np.power(Grd_w,2))
    b_v=b_v+np.multiply(1-beta,np.power(Grd_b,2))
    w_v_corrected=1/(np.power(w_v+eps,0.5))
    b_v_corrected=1/(np.power(b_v+eps,0.5))
    upd_w=np.multiply(w_v_corrected,Grd_w)*Eta
    upd_b=np.multiply(b_v_corrected,Grd_b)*Eta
    return (upd_w,upd_b)
  def Adam(self,Grd_w,Grd_b,Eta,t):
    w_m,b_m=0,0
    w_v,b_v=0,0
    beta1,beta2=0.9,0.99
    eps=1e-8
    w_v=np.multiply(beta2,w_v)
    b_v=np.multiply(beta2,b_v)
    w_v=w_v+np.multiply(1-beta2,np.power(Grd_w,2))
    b_v=b_v+np.multiply(1-beta2,np.power(Grd_b,2))
    w_v=w_v/(1-np.power(beta2,t+1))
    b_v=b_v/(1-np.power(beta2,t+1))
    w_m=np.multiply(beta1,w_m)
    b_m=np.multiply(beta1,b_m)
    w_m=w_m+np.multiply(1-beta1,Grd_w)
    b_m=b_m+np.multiply(1-beta1,Grd_b)
    w_m=w_m/(1-np.power(beta1,t+1))
    b_m=b_m/(1-np.power(beta1,t+1))
    upd_w=(Eta/np.power(w_v+eps,0.5))*w_m
    upd_b=(Eta/np.power(b_v+eps,0.5))*b_m

    return (upd_w,upd_b)
  def Nadam(self,Grd_w,Grd_b,Eta,t):
    w_m,b_m=0,0
    w_v,b_v=0,0
    beta1,beta2=0.9,0.99
    eps=1e-8
    w_v=np.multiply(beta2,w_v)
    b_v=np.multiply(beta2,b_v)
    w_v=w_v+np.multiply(1-beta2,np.power(Grd_w,2))
    b_v=b_v+np.multiply(1-beta2,np.power(Grd_b,2))
    w_m=np.multiply(beta1,w_m)
    b_m=np.multiply(beta1,b_m)
    w_m=w_m+np.multiply(1-beta1,Grd_w)
    b_m=b_m+np.multiply(1-beta1,Grd_b)
    w_m=np.multiply(beta1,w_m)/(1-beta1)+np.multiply(beta1,Grd_w)/(1-beta1)
    b_m=np.multiply(beta1,b_m)/(1-beta1)+np.multiply(beta1,Grd_b)/(1-beta1)
    w_v=np.multiply(np.multiply(beta2,w_v),1/(1-beta2))
    b_v=np.multiply(np.multiply(beta2,b_v),1/(1-beta2))
    upd_w=(Eta/np.power(w_v+eps,0.5))*w_m
    upd_b=(Eta/np.power(b_v+eps,0.5))*b_m 
                                
    return (upd_w,upd_b)
  def Nag(self,gamma,Grd_w,Grd_b,Eta,prev_w,prev_b):
    w_v,b_v=0,0
    w_v=np.multiply(gamma,prev_w)+np.multiply(Eta,Grd_w)
    b_v=np.multiply(gamma,prev_b)+np.multiply(Eta,Grd_b)
    prev_w=w_v 
    prev_b=b_v
    w_v=np.multiply(gamma,prev_w)
    b_v=np.multiply(gamma,prev_b)
    return w_v,b_v





  #Main_Training_Algo():It combines Feed forward neural Network and Back Propogation with optimizers
  def Main_Training_Algo(self,X_Valid,Y_Valid,X_Train,Y_Train,optimizer='Momentum',NEpochs=5,Batch_Size=16,Momentum=0.9,Eta=0.01,Anneal=False):
    
    etap=Eta
    Loss_in_Validation=[]
    Loss_in_Training=[]
    Loss_in_Test=[]
    update_weight=[]
    update_biases=[]
    prev_w=[np.zeros((sizej,sizei)) for sizei,sizej in zip(self.number_of_neurons[:-1],self.number_of_neurons[1:])]
    prev_b=[np.zeros((sizei,1)) for sizei in self.number_of_neurons[1:]]
    t=0
    #loop through each epochs
    for Ep in range(NEpochs):
      Loss_in_batch=0
      #loop through all the batch size
      for start in range(0,X_Train.shape[0],Batch_Size):

        X_Train_curr_batch=X_Train[start:start+Batch_Size]
        Y_Train_curr_batch=Y_Train[start:start+Batch_Size]
        
        #Feeding the network with Current batch data
        Curr_Batch_Prediction=self.Feed_Network(X_Train_curr_batch)
        #Backpropogating the data
        (Gradient_wrt_biases,Gradient_wrt_weight,Gradient_wrt_layers)=self.Backward_Pass(X_Train_curr_batch,Y_Train_curr_batch)

        #Check for optimizer
        if optimizer=='GD':
          update_weight,update_biases=self.GD(Gradient_wrt_weight,Gradient_wrt_biases,Eta)
        elif optimizer=='Momentum':
          update_weight,update_biases=self.Momentum(Momentum,Gradient_wrt_weight,Gradient_wrt_biases,Eta,prev_w,prev_b)
          prev_w=update_weight
          prev_b=update_biases
        elif optimizer=='RMSprop':
          update_weight,update_biases=self.RMSprop(Gradient_wrt_weight,Gradient_wrt_biases,Eta)
        elif optimizer=='Adam':
          update_weight,update_biases=self.Adam(Gradient_wrt_weight,Gradient_wrt_biases,Eta,t)
        elif optimizer=='Nadam':
          update_weight,update_biases=self.Nadam(Gradient_wrt_weight,Gradient_wrt_biases,Eta,t)
        elif optimizer=='Nag':
          update_weight,update_biases=self.Nag(Momentum,Gradient_wrt_weight,Gradient_wrt_biases,Eta,prev_w,prev_b)
          prev_w=update_weight
          prev_b=update_biases
          
        t=t+1

        
        self.Weights=self.Weights-update_weight
        self.Biases=self.Biases-update_biases
        if Anneal == True:
          Eta = ((0.99)**(start+3.5))*etap
        #Claculating the CE/MSE batch Loss
        if self.Loss_Function=='CE':
          Loss_in_batch=Loss_in_batch+log_loss(Y_Train_curr_batch.T,Curr_Batch_Prediction)
        elif self.Loss_Function=='MSE':
          Loss_in_batch=Loss_in_batch+(mean_squared_error(Y_Train_curr_batch.T,Curr_Batch_Prediction))          
      
      #Loss over Train data
      Loss_in_Training.append(Loss_in_batch)
      #Prediction over Train data
      Train_Prediction=self.Feed_Network(X_Train)
      print("Epoch:",Ep)
      print("Train Accuracy",self.Get_Accuracy(Y_Train.T,Train_Prediction))
      print("Loss in Training",Loss_in_Training[Ep])
      #Prediction over Validation data
      Validation_Predcition=self.Feed_Network(X_Valid)
      #Loss over Validation data
      if self.Loss_Function=='CE':
        Loss_in_Validation.append(log_loss(Y_Valid.T,Validation_Predcition))
      elif self.Loss_Function=='MSE':
        Loss_in_Validation.append(mean_squared_error(Y_Valid.T,Validation_Predcition))
      print("Validation Accuracy",self.Get_Accuracy(Y_Valid.T,Validation_Predcition))
      print("Loss in Validation",Loss_in_Validation[Ep])
      #Test accuracy and loss
      Test_Prediction=self.Feed_Network(X_Test)
      self.Get_Accuracy(Y_Test.T,Test_Prediction)
      if self.Loss_Function=='CE':
        Loss_in_Test.append(log_loss(Y_Test.T,Test_Prediction))
      elif self.Loss_Function=='MSE':
        Loss_in_Test.append(mean_squared_error(Y_Test.T,Test_Prediction))

  #Main_Training_Algo():It combines Feed forward neural Network and Back Propogation with optimizers and plots the graphs
  def Main_Training_Algo_plot(self,X_Valid,Y_Valid,X_Train,Y_Train,optimizer='Momentum',NEpochs=5,Batch_Size=16,Momentum=0.9,Eta=0.01,anneal=False):

    etap=Eta
    Loss_in_Validation=[]
    Loss_in_Training=[]
    update_weight=[]
    update_biases=[]
    prev_w=[np.zeros((sizej,sizei)) for sizei,sizej in zip(self.number_of_neurons[:-1],self.number_of_neurons[1:])]
    prev_b=[np.zeros((x,1)) for x in self.number_of_neurons[1:]]
    t=0
    for Ep in range(NEpochs):
      Loss_in_batch=0

      for start in range(0,X_Train.shape[0],Batch_Size):

        X_Train_curr_batch=X_Train[start:start+Batch_Size]
        Y_Train_curr_batch=Y_Train[start:start+Batch_Size]
        

        Curr_Batch_Prediction=self.Feed_Network(X_Train_curr_batch)

        (Gradient_wrt_biases,Gradient_wrt_weight,Gradient_wrt_layers)=self.Backward_Pass(X_Train_curr_batch,Y_Train_curr_batch)


        if optimizer=='GD':
          update_weight,update_biases=self.GD(Gradient_wrt_weight,Gradient_wrt_biases,Eta)
        elif optimizer=='Momentum':
          update_weight,update_biases=self.Momentum(Momentum,Gradient_wrt_weight,Gradient_wrt_biases,Eta,prev_w,prev_b)
          prev_w=update_weight
          prev_b=update_biases
        elif optimizer=='RMSprop':
          update_weight,update_biases=self.RMSprop(Gradient_wrt_weight,Gradient_wrt_biases,Eta)
        elif optimizer=='Adam':
          update_weight,update_biases=self.Adam(Gradient_wrt_weight,Gradient_wrt_biases,Eta,t)
        elif optimizer=='Nadam':
          update_weight,update_biases=self.Nadam(Gradient_wrt_weight,Gradient_wrt_biases,Eta,t)
        elif optimizer=='Nag':
          update_weight,update_biases=self.Nag(Momentum,Gradient_wrt_weight,Gradient_wrt_biases,Eta,prev_w,prev_b)
          prev_w=update_weight
          prev_b=update_biases
          
        t=t+1

        
        self.Weights=self.Weights-update_weight
        self.Biases=self.Biases-update_biases
        if anneal == True:
          Eta = ((0.99)**(start+3.5))*etap

        if self.Loss_Function=='CE':
          Loss_in_batch=Loss_in_batch+log_loss(Y_Train_curr_batch.T,Curr_Batch_Prediction)
        elif self.Loss_Function=='MSE':
          Loss_in_batch=Loss_in_batch+(mean_squared_error(Y_Train_curr_batch.T,Curr_Batch_Prediction))          
      

      Loss_in_Training.append(Loss_in_batch)
      Train_Prediction=self.Feed_Network(X_Train)
      print("Epoch:",Ep)
      print("Train Accuracy",self.Get_Accuracy(Y_Train.T,Train_Prediction))
      print("Loss in Training",Loss_in_Training[Ep])
      
      Validation_Predcition=self.Feed_Network(X_Valid)
      if self.Loss_Function=='CE':
        Loss_in_Validation.append(log_loss(Y_Valid.T,Validation_Predcition))
      elif self.Loss_Function=='MSE':
        Loss_in_Validation.append(mean_squared_error(Y_Valid.T,Validation_Predcition))
      print("Validation Accuracy",self.Get_Accuracy(Y_Valid.T,Validation_Predcition))
      print("Loss in Validation",Loss_in_Validation[Ep])
      wandb.log({"val_loss": Loss_in_Training[Ep]/10000, \
                       "val_acc": self.Get_Accuracy(Y_Train.T,Train_Prediction), \
                       "test_loss": Loss_in_Validation[Ep]/10000, \
                       "test_acc": self.Get_Accuracy(Y_Valid.T,Validation_Predcition), \
                       "epoch":Ep})



#Driver Code
n_classes = 10 #number of classes
loss_function = 'CE'#Loss Fucntion {CE,MSE}
seed =8790
AC_fnc='Relu' #Activation Fucntion used for hidden layer are {Tanh,Relu,Sigmoid}
Activation_gx = [AC_fnc,AC_fnc,AC_fnc,AC_fnc,'SoftMax']#Add One more entry to increase the no of layers
neurons = 64   #layer size(no. of neurons)
Layers = [784, neurons,neurons ,neurons,neurons, n_classes] #layers initialization
#Weight_init="Xavier"
Weight_init="Random"
#Network is the object of class Feed_Forward_NeuralNetwork() 
Network=Feed_Forward_NeuralNetwork(Layers,Activation_gx,loss_function,seed,Weight_init)
#optimizer used are {Momentum,Nag,Adam,Nadam,GD,RMSprop}
print(Network.Main_Training_Algo(X_Valid,Y_Valid,X_Train,Y_Train,optimizer='Nadam',NEpochs=20,Batch_Size=64,Momentum=0.9,Eta=0.001,Anneal=True))
Y_test_pred=Network.Feed_Network(X_Test)
print("Test Acc:", Network.Get_Accuracy(Y_Test.T,Y_test_pred))
print("Loss In Test",log_loss(Y_Test.T,Y_test_pred))

def Main_Plots():
    sweep_config = {"name": "complete-sweep", "method": "grid"}
    sweep_config["metric"] = {"name": "accuracy", "goal": "minimize"}
    parameters_dict = {
                    "num_epochs": {"values": [5,10,20]}, \
                    "Layer_size": {"values": [32,64,128]}, \
                    "learning_rate": {"values": [1e-3,1e-4]}, \
                    "N_hl": {"values": [4]}, \
                    "optimizer": {"values": ["Momentum","GD","Nadam","Nag","RMSProp","Adam"]}, \
                    "batch_size": {"values": [32,64,128]}, \
                    "weight_init": {"values": ["Random","Xavier"]} , \
                    "activation": {"values": [ "Tanh","Sigmoid","Relu"]}, \
                    "loss": {"values": ["CE","MSE"]}, \
                      }
    sweep_config["parameters"] = parameters_dict
    global accuracy
    accuracy=0
    def train_nn(config = sweep_config):
        global accuracy
        with wandb.init(config = config):
            config = wandb.init().config
            wandb.run.name = "e_{}_loss_{}_opt_{}_bs_{}_init_{}_ac_{}".format(config.num_epochs,\
                                                                          config.loss,\
                                                                          config.optimizer,\
                                                                          config.batch_size,\
                                                                          config.weight_init,\
                                                                          config.activation,\
                                                                          )






            n_classes = 10
            seed =8790
            loss_function = config.loss
            Activation_gx = [config.activation, config.activation,config.activation,config.activation, 'SoftMax']
            neurons=config.Layer_size
            Layers = [784, neurons,neurons,neurons,neurons, n_classes]
            Network=Feed_Forward_NeuralNetwork(Layers,Activation_gx,loss_function,seed,Weight_init=config.weight_init) 
            Network.Main_Training_Algo_plot(X_Valid,Y_Valid,X_Train,Y_Train,optimizer=config.optimizer,NEpochs=config.num_epochs,Batch_Size=config.batch_size,Momentum=0.9,Eta=config.learning_rate,anneal=True)
            Validation_Predcition=Network.Feed_Network(X_Valid)
            accuracy=Network.Get_Accuracy(Y_Valid.T,Validation_Predcition)

            wandb.log({"accuracy": accuracy})
    sweep_id = wandb.sweep(sweep_config, project = "Main_Plots")
    wandb.agent(sweep_id, function = train_nn)

Main_Plots() #To create all the main plots

def ConfusionMatrix():
  import matplotlib.pyplot as plt

  import plotly
  import seaborn as sns
  from sklearn.metrics import confusion_matrix
  sweep_config = {"name": "best-sweep", "method": "grid"}
  sweep_config["metric"] = {"name": "loss", "goal": "minimize"}
  parameters_dict = {
                    "num_epochs": {"values": [20]}, \
                    "Layer_size": {"values": [64]}, \
                    "learning_rate": {"values": [1e-3]}, \
                    "N_hl": {"values": [4]}, \
                    "optimizer": {"values": ["Nadam"]}, \
                    "batch_size": {"values": [64]}, \
                    "weight_init": {"values": ["Random"]} , \
                    "activation": {"values": [ "Tanh"]}, \
                    "loss": {"values": ["CE"]}, \
                      }
  sweep_config["parameters"] = parameters_dict
  global accuracy
  accuracy=0

  ####################################################################
  def train_nn(config = sweep_config):
      global accuracy
      with wandb.init(config = config):
          config = wandb.init().config
          wandb.run.name = "e_{}_loss_{}_opt_{}_bs_{}_init_{}_ac_{}".format(config.num_epochs,\
                                                                          config.loss,\
                                                                          config.optimizer,\
                                                                          config.batch_size,\
                                                                          config.weight_init,\
                                                                          config.activation,\
                                                                          )

        
        
        
        
          n_classes = 10
          seed =8790
          loss_function = config.loss
          Activation_gx = [config.activation, config.activation,config.activation,config.activation, 'SoftMax']
          neurons =config.Layer_size
          Layers = [784, neurons,neurons ,neurons,neurons, n_classes]
          Network=Feed_Forward_NeuralNetwork(Layers,Activation_gx,loss_function,seed,config.weight_init) 
          Network.Main_Training_Algo_plot(X_Valid,Y_Valid,X_Train,Y_Train,config.optimizer,config.num_epochs,config.batch_size,Momentum=0.9,Eta=config.learning_rate,anneal=True)
          Y_test_pred=Network.Feed_Network(X_Test)


          E=np.argmax(Y_Test.T,axis=0)
          T=np.argmax(Y_test_pred,axis=0)
          cf_matrix = confusion_matrix(E,T)
          class_names=["TshirtðŸ‘•","TrouserðŸ‘–","PulloverðŸ§¥","DressðŸ‘—","CoatðŸ¥¼","SandalðŸ‘¡","ShirtðŸ‘”","SneakerðŸ‘Ÿ","BagðŸ‘œ","AnkleBootðŸ¥¾"]
          wandb.log({"my_conf_mat_id" : wandb.plot.confusion_matrix( preds=T, y_true=E,class_names=class_names)})

  ####################################################################
  sweep_id = wandb.sweep(sweep_config, project = "Best_Plot")
  wandb.agent(sweep_id, function = train_nn)
  #################################################################### [markdown]

ConfusionMatrix()#To create confusion matrix for best Hyperparameter config.

#Question 1 Plotting the Examples
from keras.datasets import fashion_mnist
import numpy as np
import wandb
import matplotlib.pyplot as plt

wandb.init(project="Samples")
[(x_train, y_train), (x_test, y_test)] = fashion_mnist.load_data()# Load the dataset
num_of_classes = 10   # Get the number of classes and their name mappings
All_class_mapping = {0: "T-shirt/top", 1: "Trouser", 2: "Pullover", 3: "Dress", 4: "Coat", 5: "Sandal", 6: "Shirt", 7: "Sneaker", 8: "Bag", 9: "Ankle boot"}
# Plotting a figure from each class
plt.figure(figsize=[12, 5])
All_img_list = []
All_class_list = []

for i in range(num_of_classes):
    position = np.argmax(y_train==i)
    image = x_train[position,:,:]
    plt.subplot(2, 5, i+1)
    plt.imshow(image)
    plt.title(All_class_mapping[i])
    All_img_list.append(image)
    All_class_list.append(All_class_mapping[i])
    
wandb.log({"Question-1": [wandb.Image(img, caption=caption) for img, caption in zip(All_img_list, All_class_list)]})